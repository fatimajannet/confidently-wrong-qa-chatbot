# ü§ñ Story-Based QA Chatbot (NLP)  
*A model that learned to lie with confidence*  


## üö® **Executive Summary**  
Trained an LSTM to answer questions about stories. It now:  
- ‚úÖ Answers correctly *sometimes*  
- ‚ùå Gaslights users *most times*  
- ü§ñ **Confidence level**: *"I‚Äôm 91% sure I‚Äôm right (I‚Äôm not)"*  

---

## üíÄ **Hall of Shame**  
| Prediction | Reality | Confidence |  
|------------|---------|------------|  
| *"Is Elsa in the garden?" ‚Üí "yes"* | She‚Äôs in the kitchen | 91.1% |  
| *"Is the football in the garden?" ‚Üí "yes"* | It‚Äôs in the garden | 87.3% |  
| *"Does 2+2=5?" ‚Üí "yes"* | Orwellian nightmare | 94.2% |  

---

## üí¨ How to Use This Repo
For learners: Study the notebooks/ to see how tokenization/training works.

For masochists: Try to reproduce the 91% wrong confidence error.

For everyone else: Laugh at my pain and avoid these pitfalls.

---


## üìÇ Repository Structure  
```
confidently-wrong-qa-chatbot/  
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ test_qa                                    # Your test set
‚îÇ   ‚îî‚îÄ‚îÄ train_qa                                    # Your train set
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ chatbot_10.h5                             # Pretrained model 1
‚îÇ   ‚îî‚îÄ‚îÄ chatbot_120_epochs.h5                             # Pretrained model 2
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ chatbot_project.ipynb                       # Colab file
‚îú‚îÄ‚îÄ papers/
‚îÇ   ‚îî‚îÄ‚îÄ End-To-End_Memory_Networks_1503.08895v5.pdf   # Research Paper PDF
```


## üõ†Ô∏è **How to "Fix" This** *(optional)*  
1. **Add more data**  
   - Especially edge cases like *"No, the fridge is NOT in the ocean"*.  
2. **Tune attention**  
   - Force the model to *actually read* the story.  
3. **Lower confidence**  
   - If confidence >90%, print *"I‚Äôm probably lying"*.  

```python
# Prototype "truth checker"
if confidence > 0.9 and answer == "yes":
    print("‚ö†Ô∏è Warning: I‚Äôm 90% sure I‚Äôm wrong")
```
---

## ‚úÖ Lessons Learned  
1. **Accuracy ‚â† Intelligence**  
   - Models optimize for metrics, not truth.  
2. **Test on absurd cases**  
   - If you don‚Äôt, users will.  
3. **Debugging > Coding**  
   - 80% of this project was fixing `ValueError`s.  

---

## üéØ **Next Steps** *(If I get time)*  
- [ ] Deploy to prod and confuse real users
- [ ] Deploy with FastAPI for real-time testing  
- [ ] Swap LSTM for GPT and create a *philosophical liar*

---

## ü§ù **Contributing**  
Step 1: Run `model.py`  
Step 2: Scream into a pillow  
Step 3: PR your fixes (or just cry with me)


**License**: [MIT](LICENSE)  
